{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Using NLTK Library"
      ],
      "metadata": {
        "id": "xWlz_d6Jlnai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rye-_6ttf3Zy",
        "outputId": "f316f76d-78b8-4708-ca11-fe5668e13bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from nltk.probability import FreqDist, MLEProbDist\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_corpus = \"Oft hath he seen that wherefore he was fain.\"\n",
        "foreign_corpus = \"Le vent souffle fort sur la mer.\""
      ],
      "metadata": {
        "id": "2FVmNZ4hgrj5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "DmrAQBpflCvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_tokens = nltk.word_tokenize(shakespeare_corpus.lower())\n",
        "foreign_tokens = nltk.word_tokenize(foreign_corpus.lower())"
      ],
      "metadata": {
        "id": "jnPxmUjPlAeg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating N-grams"
      ],
      "metadata": {
        "id": "2-DEEeg9lFUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate n-grams\n",
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Creating unigrams, bigrams, and trigrams\n",
        "shakespeare_unigrams = generate_ngrams(shakespeare_tokens, 1)\n",
        "shakespeare_bigrams = generate_ngrams(shakespeare_tokens, 2)\n",
        "shakespeare_trigrams = generate_ngrams(shakespeare_tokens, 3)\n",
        "\n",
        "foreign_unigrams = generate_ngrams(foreign_tokens, 1)\n",
        "foreign_bigrams = generate_ngrams(foreign_tokens, 2)\n",
        "foreign_trigrams = generate_ngrams(foreign_tokens, 3)\n",
        "\n",
        "print(\"Shakespeare Unigrams:\", shakespeare_unigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Bigrams:\", shakespeare_bigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Trigrams:\", shakespeare_trigrams)\n",
        "print('\\n')\n",
        "\n",
        "print(\"Foreign Unigrams:\", foreign_unigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Bigrams:\", foreign_bigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Trigrams:\", foreign_trigrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufV1N3bhgzQ4",
        "outputId": "6b97f050-4e99-4302-c3a6-aaf979137ba2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare Unigrams: [('oft',), ('hath',), ('he',), ('seen',), ('that',), ('wherefore',), ('he',), ('was',), ('fain',), ('.',)]\n",
            "\n",
            "\n",
            "Shakespeare Bigrams: [('oft', 'hath'), ('hath', 'he'), ('he', 'seen'), ('seen', 'that'), ('that', 'wherefore'), ('wherefore', 'he'), ('he', 'was'), ('was', 'fain'), ('fain', '.')]\n",
            "\n",
            "\n",
            "Shakespeare Trigrams: [('oft', 'hath', 'he'), ('hath', 'he', 'seen'), ('he', 'seen', 'that'), ('seen', 'that', 'wherefore'), ('that', 'wherefore', 'he'), ('wherefore', 'he', 'was'), ('he', 'was', 'fain'), ('was', 'fain', '.')]\n",
            "\n",
            "\n",
            "Foreign Unigrams: [('le',), ('vent',), ('souffle',), ('fort',), ('sur',), ('la',), ('mer',), ('.',)]\n",
            "\n",
            "\n",
            "Foreign Bigrams: [('le', 'vent'), ('vent', 'souffle'), ('souffle', 'fort'), ('fort', 'sur'), ('sur', 'la'), ('la', 'mer'), ('mer', '.')]\n",
            "\n",
            "\n",
            "Foreign Trigrams: [('le', 'vent', 'souffle'), ('vent', 'souffle', 'fort'), ('souffle', 'fort', 'sur'), ('fort', 'sur', 'la'), ('sur', 'la', 'mer'), ('la', 'mer', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "3MBzgxJjlINJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_freq = FreqDist(shakespeare_bigrams)\n",
        "foreign_bigram_freq = FreqDist(foreign_bigrams)\n",
        "print('Shakespeare'  ,shakespeare_bigram_freq)\n",
        "print('Foreign Language  ',foreign_bigram_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-mvlWpgzuR",
        "outputId": "1be21a17-455a-4019-bca8-be87ec37391f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare <FreqDist with 9 samples and 9 outcomes>\n",
            "Foreign Language   <FreqDist with 7 samples and 7 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Calculating Probabilities using MLE"
      ],
      "metadata": {
        "id": "dQIBWecklLyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_prob = MLEProbDist(shakespeare_bigram_freq)\n",
        "foreign_bigram_prob = MLEProbDist(foreign_bigram_freq)\n",
        "print('Shakespeare  ',shakespeare_bigram_prob)\n",
        "print('Foreign Language  ',foreign_bigram_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkwFlW5_g2RI",
        "outputId": "dae36389-77a5-4440-8c67-a638b1815082"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare   <MLEProbDist based on 9 samples>\n",
            "Foreign Language   <MLEProbDist based on 7 samples>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the next word"
      ],
      "metadata": {
        "id": "G_glDKWtlNRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(word, bigram_freq, bigram_prob):\n",
        "    candidates = [(pair, bigram_prob.prob(pair)) for pair in bigram_freq if pair[0] == word]\n",
        "    return max(candidates, key=lambda x: x[1])[0][1] if candidates else None\n",
        "\n",
        "\n",
        "print(\"Most likely next word after 'he' (Shakespeare):\", predict_next_word('he', shakespeare_bigram_freq, shakespeare_bigram_prob))\n",
        "print('\\n')\n",
        "print(\"Most likely next word after 'le' (Foreign):\", predict_next_word('le', foreign_bigram_freq, foreign_bigram_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2OS6ufTg5W5",
        "outputId": "bbbcb011-3bfe-408b-d751-d549e7ed4e1a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely next word after 'he' (Shakespeare): seen\n",
            "\n",
            "\n",
            "Most likely next word after 'le' (Foreign): vent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Spacy Library"
      ],
      "metadata": {
        "id": "Cq1PRh5vieSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COlj2Jv4iLz5",
        "outputId": "d841cdb6-3b19-4953-e8f6-a457a3084532"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from fr-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist, MLEProbDist\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJaQ_VxEii6Y",
        "outputId": "4971a46f-f6fd-4ac5-a30f-d347aeacce5c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_corpus = \"Oft hath he seen that wherefore he was fain.\"\n",
        "foreign_corpus = \"Le vent souffle fort sur la mer.\"\n"
      ],
      "metadata": {
        "id": "XI2hpdoziqoR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "fjh7p1LyjvE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_tokens = [token.text.lower() for token in nlp_en(shakespeare_corpus)]\n",
        "foreign_tokens = [token.text.lower() for token in nlp_fr(foreign_corpus)]\n"
      ],
      "metadata": {
        "id": "MbWjqo3oi24g"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating N-grams"
      ],
      "metadata": {
        "id": "DPmfpXf5jwnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Creating unigrams, bigrams, and trigrams\n",
        "shakespeare_unigrams = generate_ngrams(shakespeare_tokens, 1)\n",
        "shakespeare_bigrams = generate_ngrams(shakespeare_tokens, 2)\n",
        "shakespeare_trigrams = generate_ngrams(shakespeare_tokens, 3)\n",
        "\n",
        "foreign_unigrams = generate_ngrams(foreign_tokens, 1)\n",
        "foreign_bigrams = generate_ngrams(foreign_tokens, 2)\n",
        "foreign_trigrams = generate_ngrams(foreign_tokens, 3)\n",
        "\n",
        "print(\"Shakespeare Unigrams:\", shakespeare_unigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Bigrams:\", shakespeare_bigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Trigrams:\", shakespeare_trigrams)\n",
        "\n",
        "print('\\n')\n",
        "print(\"Foreign Unigrams:\", foreign_unigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Bigrams:\", foreign_bigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Trigrams:\", foreign_trigrams)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhVSnrFAi3Fp",
        "outputId": "6532d156-e7b0-4c00-aa27-45a5e852a622"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare Unigrams: [('oft',), ('hath',), ('he',), ('seen',), ('that',), ('wherefore',), ('he',), ('was',), ('fain',), ('.',)]\n",
            "\n",
            "\n",
            "Shakespeare Bigrams: [('oft', 'hath'), ('hath', 'he'), ('he', 'seen'), ('seen', 'that'), ('that', 'wherefore'), ('wherefore', 'he'), ('he', 'was'), ('was', 'fain'), ('fain', '.')]\n",
            "\n",
            "\n",
            "Shakespeare Trigrams: [('oft', 'hath', 'he'), ('hath', 'he', 'seen'), ('he', 'seen', 'that'), ('seen', 'that', 'wherefore'), ('that', 'wherefore', 'he'), ('wherefore', 'he', 'was'), ('he', 'was', 'fain'), ('was', 'fain', '.')]\n",
            "\n",
            "\n",
            "Foreign Unigrams: [('le',), ('vent',), ('souffle',), ('fort',), ('sur',), ('la',), ('mer',), ('.',)]\n",
            "\n",
            "\n",
            "Foreign Bigrams: [('le', 'vent'), ('vent', 'souffle'), ('souffle', 'fort'), ('fort', 'sur'), ('sur', 'la'), ('la', 'mer'), ('mer', '.')]\n",
            "\n",
            "\n",
            "Foreign Trigrams: [('le', 'vent', 'souffle'), ('vent', 'souffle', 'fort'), ('souffle', 'fort', 'sur'), ('fort', 'sur', 'la'), ('sur', 'la', 'mer'), ('la', 'mer', '.')]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "MCXI_BvOj1Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_freq = FreqDist(shakespeare_bigrams)\n",
        "foreign_bigram_freq = FreqDist(foreign_bigrams)\n",
        "print('Shakespeare  ',shakespeare_bigram_freq)\n",
        "print('Foreign Language  ',foreign_bigram_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvqbU1WBi5_g",
        "outputId": "22e08dc1-be03-4edb-a360-907bc05472fd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare   <FreqDist with 9 samples and 9 outcomes>\n",
            "Foreign Language   <FreqDist with 7 samples and 7 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Probabilities using MLE"
      ],
      "metadata": {
        "id": "zBtBvILOj32g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_prob = MLEProbDist(shakespeare_bigram_freq)\n",
        "foreign_bigram_prob = MLEProbDist(foreign_bigram_freq)\n",
        "print('Shakespeare  ',shakespeare_bigram_prob)\n",
        "print('Foreign Language  ',foreign_bigram_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CFNAyoSi9F5",
        "outputId": "070db883-c0aa-4950-ae3b-2b9af357385c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare   <MLEProbDist based on 9 samples>\n",
            "Foreign Language   <MLEProbDist based on 7 samples>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the most likely next word"
      ],
      "metadata": {
        "id": "e9UWjZcOj_VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(word, bigram_freq, bigram_prob):\n",
        "    candidates = [(pair, bigram_prob.prob(pair)) for pair in bigram_freq if pair[0] == word]\n",
        "    return max(candidates, key=lambda x: x[1])[0][1] if candidates else None\n",
        "\n",
        "print(\"Most likely next word after 'he' (Shakespeare):\", predict_next_word('he', shakespeare_bigram_freq, shakespeare_bigram_prob))\n",
        "print('\\n')\n",
        "print(\"Most likely next word after 'le' (Foreign):\", predict_next_word('le', foreign_bigram_freq, foreign_bigram_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aq6CF96i_Oo",
        "outputId": "62d62a92-9e57-4e97-e096-739d80bde677"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely next word after 'he' (Shakespeare): seen\n",
            "\n",
            "\n",
            "Most likely next word after 'le' (Foreign): vent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Textblob Library"
      ],
      "metadata": {
        "id": "JkXfl_ualj7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.probability import FreqDist, MLEProbDist"
      ],
      "metadata": {
        "id": "en0gTMXqjGgY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_corpus = \"Oft hath he seen that wherefore he was fain.\"\n",
        "foreign_corpus = \"Le vent souffle fort sur la mer.\""
      ],
      "metadata": {
        "id": "XKW0XHc7l2BA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "JzwGOYY9l5xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_tokens = TextBlob(shakespeare_corpus.lower()).words\n",
        "foreign_tokens = TextBlob(foreign_corpus.lower()).words"
      ],
      "metadata": {
        "id": "SZPs1jzdl3X4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating N-grams"
      ],
      "metadata": {
        "id": "uRy6hcEjmIl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(tokens, n):\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "# Creating unigrams, bigrams, and trigrams\n",
        "shakespeare_unigrams = generate_ngrams(shakespeare_tokens, 1)\n",
        "shakespeare_bigrams = generate_ngrams(shakespeare_tokens, 2)\n",
        "shakespeare_trigrams = generate_ngrams(shakespeare_tokens, 3)\n",
        "\n",
        "foreign_unigrams = generate_ngrams(foreign_tokens, 1)\n",
        "foreign_bigrams = generate_ngrams(foreign_tokens, 2)\n",
        "foreign_trigrams = generate_ngrams(foreign_tokens, 3)\n",
        "\n",
        "print(\"Shakespeare Unigrams:\", shakespeare_unigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Bigrams:\", shakespeare_bigrams)\n",
        "print('\\n')\n",
        "print(\"Shakespeare Trigrams:\", shakespeare_trigrams)\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print(\"Foreign Unigrams:\", foreign_unigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Bigrams:\", foreign_bigrams)\n",
        "print('\\n')\n",
        "print(\"Foreign Trigrams:\", foreign_trigrams)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BJhteZul5QY",
        "outputId": "b82ec70b-e138-4758-fef8-ac52aae6c836"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare Unigrams: [('oft',), ('hath',), ('he',), ('seen',), ('that',), ('wherefore',), ('he',), ('was',), ('fain',)]\n",
            "\n",
            "\n",
            "Shakespeare Bigrams: [('oft', 'hath'), ('hath', 'he'), ('he', 'seen'), ('seen', 'that'), ('that', 'wherefore'), ('wherefore', 'he'), ('he', 'was'), ('was', 'fain')]\n",
            "\n",
            "\n",
            "Shakespeare Trigrams: [('oft', 'hath', 'he'), ('hath', 'he', 'seen'), ('he', 'seen', 'that'), ('seen', 'that', 'wherefore'), ('that', 'wherefore', 'he'), ('wherefore', 'he', 'was'), ('he', 'was', 'fain')]\n",
            "\n",
            "\n",
            "Foreign Unigrams: [('le',), ('vent',), ('souffle',), ('fort',), ('sur',), ('la',), ('mer',)]\n",
            "\n",
            "\n",
            "Foreign Bigrams: [('le', 'vent'), ('vent', 'souffle'), ('souffle', 'fort'), ('fort', 'sur'), ('sur', 'la'), ('la', 'mer')]\n",
            "\n",
            "\n",
            "Foreign Trigrams: [('le', 'vent', 'souffle'), ('vent', 'souffle', 'fort'), ('souffle', 'fort', 'sur'), ('fort', 'sur', 'la'), ('sur', 'la', 'mer')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "E8pAFBSLmM7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_freq = FreqDist(shakespeare_bigrams)\n",
        "foreign_bigram_freq = FreqDist(foreign_bigrams)\n",
        "print('Shakespeare  ',shakespeare_bigram_freq)\n",
        "print('Foreign Language  ',foreign_bigram_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv0TJp9YmHaA",
        "outputId": "673041fc-877e-48b5-aa2f-9aebeaee6d0b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare   <FreqDist with 8 samples and 8 outcomes>\n",
            "Foreign Language   <FreqDist with 6 samples and 6 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Probabilities using MLE"
      ],
      "metadata": {
        "id": "YV0H9UrhmSxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_bigram_prob = MLEProbDist(shakespeare_bigram_freq)\n",
        "foreign_bigram_prob = MLEProbDist(foreign_bigram_freq)\n",
        "print('Shakespeare  ',shakespeare_bigram_prob)\n",
        "print('Foreign Language  ',foreign_bigram_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8xowaUEmQbR",
        "outputId": "8c2f093b-7db6-41c2-9833-a7860d060948"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shakespeare   <MLEProbDist based on 8 samples>\n",
            "Foreign Language   <MLEProbDist based on 6 samples>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the most likely next word"
      ],
      "metadata": {
        "id": "eJdewQW2maTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(word, bigram_freq, bigram_prob):\n",
        "    candidates = [(pair, bigram_prob.prob(pair)) for pair in bigram_freq if pair[0] == word]\n",
        "    return max(candidates, key=lambda x: x[1])[0][1] if candidates else None\n",
        "print(\"Most likely next word after 'he' (Shakespeare):\", predict_next_word('he', shakespeare_bigram_freq, shakespeare_bigram_prob))\n",
        "print('\\n')\n",
        "print(\"Most likely next word after 'le' (Foreign):\", predict_next_word('le', foreign_bigram_freq, foreign_bigram_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyUnsmBrmWXQ",
        "outputId": "a182675e-c31d-4d86-be56-597c63698fdd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely next word after 'he' (Shakespeare): seen\n",
            "\n",
            "\n",
            "Most likely next word after 'le' (Foreign): vent\n"
          ]
        }
      ]
    }
  ]
}